# EMBED_AUTH_TOKEN=wuREk2-A7G"uB-EdUVaM-uFAH5i-gEHuHe-T5eTEB-(e
X_API_TOKEN=1
# X_API_TOKEN=fztEud-dNCNgC-#5G9pV-kfx6'P-Naf9Dc-wtqREK-Bd
PORT=3004
HOST=0.0.0.0

# EMBEDDING_MODEL_NAME = "hkunlp/instructor-xl" # Uses 5 GB of VRAM (Most Accurate of all models)
# EMBEDDING_MODEL_NAME = "intfloat/e5-large-v2" # Uses 1.5 GB of VRAM (A little less accurate than instructor-large)
# EMBEDDING_MODEL_NAME = "intfloat/e5-base-v2" # Uses 0.5 GB of VRAM (A good model for lower VRAM GPUs)
# EMBEDDING_MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2" Uses 0.2 GB of VRAM (Less accurate but fastest - only requires 150mb of vram)

# EMBEDDING_MODEL_NAME = "hkunlp/instructor-large" #  has'nt been trained on Russian
# EMBEDDING_MODEL_NAME = "intfloat/e5-large-v2" # Uses 1.5 GB of VRAM (A little less accurate than instructor-large)
# EMBEDDING_MODEL_NAME = "intfloat/e5-base-v2" # Uses 0.5 GB of VRAM (A good model for lower VRAM GPUs)
# EMBEDDING_MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2" Uses 0.2 GB of VRAM (Less accurate but fastest - only requires 150mb of vram)

# EMBEDDING_MODEL_NAME=sentence-transformers/multi-qa-mpnet-base-dot-v1
# EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

EMBEDDING_MODEL_NAME=sentence-transformers/all-distilroberta-v1
MODELS_PATH=model-store
DEVICE=cpu


