version: '3.8'

services:
  vllm:
    container_name: vllm
    # image: ghcr.io/mistralai/mistral-src/vllm:latest
    # image: docker.io/vllm/vllm-openai:latest
    image: vllm/vllm-openai:latest
    restart: always
    environment:
      HOST: ${HOST}
      PORT: ${MM_PORT}
      DEVICE: ${DEVICE}
      COMPLETION_MODEL_NAME: ${COMPLETION_MODEL_NAME}
      CUDA_HOME: "/usr/local/cuda"
      NVIDIA_VISIBLE_DEVICES: all
      TP_SIZE: ${TP_SIZE}
      CHAT_TEMPLATE: ${CHAT_TEMPLATE}
    ipc: host
    command:
      - "--host=${HOST}"
      - "--port=${MM_PORT}"
      - "--model=/model-store/${COMPLETION_MODEL_NAME}"
      # - "--tensor-parallel-size=${TP_SIZE}"
      # - "--trust_remote_code=True"
      # - "--quantization=gptq"
      # - "--dtype=float16"
      - "--quantization=awq"
      - "--dtype=auto"
      # - "--chat-template=${CHAT_TEMPLATE}"
    volumes:
      - "../../model-store/:/model-store"
    ports:
      - ${MM_PORT}:${MM_PORT}


    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]      
    networks:
      - llm-net
networks:
  llm-net:
    name: llm-net
    external: true              

# usage: api_server.py [-h] [--host HOST] [--port PORT] [--allow-credentials]
#                      [--allowed-origins ALLOWED_ORIGINS]
#                      [--allowed-methods ALLOWED_METHODS]
#                      [--allowed-headers ALLOWED_HEADERS]
#                      [--served-model-name SERVED_MODEL_NAME]
#                      [--chat-template CHAT_TEMPLATE]
#                      [--response-role RESPONSE_ROLE] [--model MODEL]
#                      [--tokenizer TOKENIZER] [--revision REVISION]
#                      [--tokenizer-revision TOKENIZER_REVISION]
#                      [--tokenizer-mode {auto,slow}] [--trust-remote-code]
#                      [--download-dir DOWNLOAD_DIR]
#                      [--load-format {auto,pt,safetensors,npcache,dummy}]
#                      [--dtype {auto,half,float16,bfloat16,float,float32}]
#                      [--max-model-len MAX_MODEL_LEN] [--worker-use-ray]
#                      [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
#                      [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
#                      [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]
#                      [--block-size {8,16,32}] [--seed SEED]
#                      [--swap-space SWAP_SPACE]
#                      [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
#                      [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
#                      [--max-num-seqs MAX_NUM_SEQS]
#                      [--max-paddings MAX_PADDINGS] [--disable-log-stats]
#                      [--quantization {awq,gptq,squeezellm,None}]
#                      [--enforce-eager]
#                      [--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]
#                      [--engine-use-ray] [--disable-log-requests]
#                      [--max-log-len MAX_LOG_LEN]